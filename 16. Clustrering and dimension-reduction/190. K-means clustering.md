- k-means 클러스터링의 기본 개념
- 어떻게 k-means 알고리즘이 동작하는지
-  k-means 알고리즘이 예제에서는 놀라운 결과를 보여주지만, 실제 응용에서는 상당히 까다로운 이유

---
## K-means objective

<span style="color:rgb(118, 147, 234)">k-means 클러스터링의 목표는 다차원 데이터를 K개의 그룹으로 묶는 것</span> 

<span style="color:rgb(116, 195, 194)">거리를 통해 그룹의 소속을 정의</span> 

<span style="color:rgb(205, 205, 81)">따라서, k-means 클러스터링은 그룹 내 사거리를 최소화하고 그룹간 거리를 최대화 하려고 시도함</span> 

---
## Example k-means clustering

![190.Pasted image 20241007082031](../pic/16.%20Clustrering%20and%20dimension-reduction/190.Pasted%20image%2020241007082031.png)

	여기서는 별 마크 중심으로 노이즈를 추가하였고, 주황색 점은 실질적인 평균

---
## Basic k-means algorithm

1. <span style="color:rgb(118, 147, 234)">k 값을 정한다.</span> 
2. <span style="color:rgb(116, 195, 194)">k개의 중심점을 데이터셋의 무작위 위치에 생성</span> 
3. <span style="color:rgb(205, 205, 81)">각 데이터 포인트에서 각 k 중심까지의 제곱거리합(오류)을 계산</span> 
4. <span style="color:rgb(236, 158, 111)">각 데이터 포인트를 가장 가까운 중심점에 할당</span> 
5. <span style="color:rgb(230, 122, 122)">모든 데이터 포인트의 평균에 새로운 중심점을 생성</span> 
6. 3-5단계를 수렴할때까지 반복

---
## Difficulties with k-means

1. <span style="color:rgb(118, 147, 234)">적절한 k 값을 사전에 알기 어려울 수 있음</span>
2. <span style="color:rgb(116, 195, 194)">적절한 k 값을 평가하는것도 어려움</span>
3. <span style="color:rgb(205, 205, 81)">다차원 클러스터링은 시각화하기 어렵거나 불가능할 수 있다.</span>
4. <span style="color:rgb(236, 158, 111)">알고리즘을 반복할 때마다 다른 결과를 낼 수 있다.</span>
5. <span style="color:rgb(230, 122, 122)">클러스터 크기가 다른 경우 최적이 아닐 수 있다.</span>
6. 모든 클러스터가 거리 기반은 아니다.

---
## Code

```python
## Create data

nPerClust = 50

# blur around centroid (std units)
blur = 1

# XY centroid locations
A = [1,1]
B = [-3,1]
C = [3,3]

# generate data
a = [A[0]+np.random.randn(nPerClust)*blur, A[1]+np.random.randn(nPerClust)*blur]
b = [B[0]+np.random.randn(nPerClust)*blur, B[1]+np.random.randn(nPerClust)*blur]
c = [C[0]+np.random.randn(nPerClust)*blur, C[1]+np.random.randn(nPerClust)*blur]

# concatanate in to list
data = np.transpose(np.concatenate((a,b,c),axis=1))

# show the data
plt.plot(data[:,0],data[:,1],'s')
plt.title('How k-means sees the data')
plt.show()
```
![190.Pasted image 20241007191636](../pic/16.%20Clustrering%20and%20dimension-reduction/190.Pasted%20image%2020241007191636.png)

```python
## k-means clustering

k = 3 # how many clusters?
kmeans = KMeans(n_clusters=k)
kmeans = kmeans.fit(data)
# group labels
groupidx = kmeans.predict(data)
#centroids
cents = kmeans.cluster_centers_

# draw lines from each data paint
lineColors = 'rgbgmrkbgm'
for i in range(0,len(data)):
    plt.plot([data[i,0],cents[groupidx[i],0]],[data[i,1],cents[groupidx[i],1]],lineColors[groupidx[i]])

# and now plot the centroid locations
plt.plot(cents[:,0],cents[:,1],'ko')

# finally, the "ground-truth" centers
plt.plot(A[0], A[1],'mp')
plt.plot(B[0], B[1],'mp')
plt.plot(C[0], C[1],'mp')

plt.show()

```
![190.Pasted image 20241007191724](../pic/16.%20Clustrering%20and%20dimension-reduction/190.Pasted%20image%2020241007191724.png)

```python
## determining the oppropriate number of clusters (qualitative)

fig, ax= plt.subplots(2,3,figsize=(7,5))
ax= ax.flatten()

for k in range(6):

    kmeans = KMeans(n_clusters=k+1).fit(data)
    groupidx = kmeans.predict(data)
    cents = kmeans.cluster_centers_

    # draw lines from each data point to the centroids of each cluster 
    for i in range(0,len(data)):
        ax[k].plot([data[i,0],cents[groupidx[i],0]],[data[i,1],cents[groupidx[i],1]],lineColors[groupidx[i]])


    # and now plot the centroid locations
    ax[k].plot(cents[:,0],cents[:,1],'ko')
    ax[k].set_xticks([])
    ax[k].set_yticks([])
    ax[k].set_title('%g clusters'%(k+1))
```
![190.Pasted image 20241007191750](../pic/16.%20Clustrering%20and%20dimension-reduction/190.Pasted%20image%2020241007191750.png)

```python
## number of clusters (quantative)

from sklearn.metrics import silhouette_samples, silhouette_score

ssds = np.zeros(7)
sils = np.zeros(7)/0

for k in range(7):
    kmeans = KMeans(n_clusters=k+1).fit(data)
    ssds[k] = np.mean(kmeans.inertia_)

    if k>0:
        s  = silhouette_samples(data,kmeans.predict(data))
        sils[k] = np.mean(s)

plt.plot(np.arange(1,8),ssds,'k^-',markerfacecolor='k')
plt.title('The elbow test')
plt.show()

plt.plot(np.arange(1,8),sils,'k^-',markerfacecolor='k')
plt.title('The silhouette test')
plt.xlabel('Number of clusters')
plt.show()
```
![190.Pasted image 20241007191814](../pic/16.%20Clustrering%20and%20dimension-reduction/190.Pasted%20image%2020241007191814.png)
![190.Pasted image 20241007191829](../pic/16.%20Clustrering%20and%20dimension-reduction/190.Pasted%20image%2020241007191829.png)
