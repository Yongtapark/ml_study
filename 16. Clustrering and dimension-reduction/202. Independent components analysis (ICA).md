- 독립성분분석(ICA)의 개념
- ICA 알고리즘의 단순화 버전 오버뷰
- 다른 다변량성분분석과의 비교
----
## ICA assumptions

<span style="color:rgb(236, 158, 111)">ICA : </span>
<span style="color:rgb(230, 122, 122)">Gaussian = BAD</span> 

	ICA는 신호가 비가우시안 분포를 따른다는 가정과, 신호들의 임의 혼합은 가우시안 분포를 따른다는 가정에서 출발

![202.Pasted image 20241015104617](../pic/16.%20Clustrering%20and%20dimension-reduction/202.Pasted%20image%2020241015104617.png)

	실제 신호는 비가우시안 분포를 따르고, 신호들의 무작위 혼합은 가우시안 분포를 따른다.

---
## How ICA works

![202.Pasted image 20241015105205](../pic/16.%20Clustrering%20and%20dimension-reduction/202.Pasted%20image%2020241015105205.png)



<span style="color:rgb(116, 195, 194)">공분산을 제거하기 위해 데이터를 화이트닝한다.</span> 

	화이트닝 : PCA에 의해 수행되는 단계, 기본적으로 공분산 행렬을 대각선 이외의 요소가 모두 0인 행렬로 변환하는것

![202.Pasted image 20241015105225](../pic/16.%20Clustrering%20and%20dimension-reduction/202.Pasted%20image%2020241015105225.png)

	화이트닝된 버전의 예시. 여전히 변수들 간의 관계가 있음을 볼 수 있음
	ICA는 이러한 관계를 찾아냄 
	하지만 데이터 자체는 모든 상관관계가 제거됨
	따라서 PC1, PC2간의 상관관계는 0


<span style="color:rgb(118, 147, 234)">선형종속성은 제거되지만, 공유된 정보는 유지됨.</span> 

<span style="color:rgb(236, 158, 111)">공유된 정보를 최소하기 위해 축을 회전(경사적으로) 시킴</span> 

![202.Pasted image 20241015110807](../pic/16.%20Clustrering%20and%20dimension-reduction/202.Pasted%20image%2020241015110807.png)

---
## Optimizing spatial filters

|                                                   | **<span style="color:rgb(118, 147, 234)">최대화</span>**                       | **<span style="color:rgb(118, 147, 234)">가정</span>**    | **<span style="color:rgb(118, 147, 234)">통계</span>** |
| ------------------------------------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------- | ---------------------------------------------------- |
| <span style="color:rgb(116, 195, 194)">ICA</span> | <span style="color:rgb(116, 195, 194)">독립성</span>                           | <span style="color:rgb(116, 195, 194)">가우시안 = 나쁨</span> | <span style="color:rgb(116, 195, 194)">기술적</span>    |
| <span style="color:rgb(236, 158, 111)">PCA</span> | <span style="color:rgb(236, 158, 111)">공분산 거듭제곱</span>                      | <span style="color:rgb(236, 158, 111)">분산 = 중요성</span>  | <span style="color:rgb(236, 158, 111)">기술적</span>    |
| <span style="color:rgb(205, 205, 81)">GED</span>  | <span style="color:rgb(205, 205, 81)">다변량 신호 대 잡음비(multivariate SNR)</span> | <span style="color:rgb(205, 205, 81)">선형 상호작용</span>    | <span style="color:rgb(205, 205, 81)">가설 기반</span>   |

---
## Code 

강사는 파이썬 내 ICA가 잘 동작하지 않는다고 언급

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import FastICA
```

```python
## Create the data

# number of data points
N = 1000

# a non-Gaussian distribution
dist1 = np.random.rand(N)

# another non-Gaussian distribution 
dist2 = np.random.rand(N)**4

# setup the figure
fig = plt.figure(constrained_layout=False,figsize=(10,8))
axs = fig.add_gridspec(2,2)

# individual distributions
ax1 = fig.add_subplot(axs[0,0])
ax1.hist(dist1,100)
ax1.set_title('Distribution 1')

ax2 = fig.add_subplot(axs[0,1])
ax2.hist(dist2,100)
ax2.set_title('Distribution 2')

# and their summed histrogram
ax3 = fig.add_subplot(axs[1,:])
ax3.hist(dist1+dist2,100)
ax3.set_title('Distribution 1+2')

plt.show()
```
![202.Pasted image 20241015140701](../pic/16.%20Clustrering%20and%20dimension-reduction/202.Pasted%20image%2020241015140701.png)

```python
## ICA

# two non-Gaussian distributions
data =np.vstack((.4*dist1+.3*dist2,.8*dist1-.7*dist2))

# ICA and scores
fastica = FastICA(max_iter=10000,tol=.0000001)
b = fastica.fit_transform(data)
iscores = b@data

# plot distributions

# IC 1
fig,ax = plt.subplots(1,2,figsize=(8,5))
ax[0].hist(iscores[0,:],100)
ax[0].set_title('IC 1')

ax[1].hist(iscores[1,:],100)
ax[1].set_title('IC 2')

plt.show()
```
![202.Pasted image 20241015140720](../pic/16.%20Clustrering%20and%20dimension-reduction/202.Pasted%20image%2020241015140720.png)


```python
## Look at the data in data space and IC space

fig,ax = plt.subplots(1,2,figsize=(8,5))

ax[0].plot(data[0,:],data[1,:],'o')
ax[0].set_title('Data space')

ax[1].plot(iscores[0,:],iscores[1,:],'o')
ax[1].set_title('IC space')
plt.show()
```
![202.Pasted image 20241015140734](../pic/16.%20Clustrering%20and%20dimension-reduction/202.Pasted%20image%2020241015140734.png)

```python
## show that the original data match the ICs

# now plot data as a function of ICs
fig,ax = plt.subplots(1,2,figsize=(8,5))

ax[0].plot(dist1,iscores[0,:],'o')
ax[0].set_xticks([])
ax[0].set_yticks([])
ax[0].set_xlabel('Original siganl')
ax[0].set_ylabel('IC1 scores')

ax[1].plot(dist1,iscores[1,:],'o')
ax[1].set_xticks([])
ax[1].set_yticks([])
ax[1].set_xlabel('Original siganl')
ax[1].set_ylabel('IC2 scores')
plt.show()
```
![202.Pasted image 20241015140747](../pic/16.%20Clustrering%20and%20dimension-reduction/202.Pasted%20image%2020241015140747.png)