- PCA의 개념
- 간단한 수학적 오버뷰
- 적절한 해석과 PCA 사용법
---
## The idea of PCA

![199.Pasted image 20241011122142](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241011122142.png)

	현재 데이터에서 중요한 축은 x,y축이 아님
	이 데이터에는 상관 구조의 특성을 훨씬 잘 포착하는 또 다른 축(주황,빨강)이 존재

	PCA의 개념은 벡터, 즉, 다변량 상관 공간의 방향을 식별하는 것

	동일 데이터는 PC(주성분) 공간에서는 아래처럼 보임
![199.Pasted image 20241011122552](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241011122552.png)

	x,y 변수를 차원으로 사용하는 대신 PC1,PC2를 차원으로 사용
	데이터 변경없이 방향만 재조정한것
	


	PCA를 적용한 후 할수 있는 일 중 하나는 데이터 압축
	데이터 압축 : 데이터 공간에서 일부 차원을 평면화 하는 개념

![199.Pasted image 20241014075716](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014075716.png)

	PC2 가 의미 없는 변동이라고 가정할 때, 의미없는 데이터이므로 압축이 가능하다.
 
	분석이 더 간단해짐 - 6차원 -> 2차원으로 줄인다면 데이터를 2차원에서 시각화 할 수 있음.
	데이터 포인트(x,y와의 관계)에서 중요하게 여기는 축은 보여주고, 중요하지 않은 축은 평면화 하는 것

---
## Covariance matrices for PCA

![199.Pasted image 20241014080916](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014080916.png)

	특정 데이터 세트에서 공분산 행렬로 전환되는 과정

---
## A matrix and its eigenvector/value pairs
-행렬과 고유벡터/고유값

	공분산 행렬을 사용하여 고유값 분해(eigen decomposition)라는 작업을 수행

![199.Pasted image 20241014080811](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014080811.png)

	고유값 분해 : 같은 개수의 행과 열을 가진 정방 행렬을 고유벡터와 고유값 세트로 분해하는 작업
	고유값 벡터는 데이터셋의 다양한 특성들을 가중치를 준 조합으로 만드는 방법


	PCA에서 우리가 하는 일은 이 고유벡터를 공분산 행렬을 생성할 때 사용된 원본 데이터에 곱하는 것

	중요한 점은 각 방향이 얼마나 중요한지 알려주는 고유값,벡터의 세트를 얻는다는 것
	이 벡터들이 주성분이 되며, 데이터를 회전시키는 방법

---
## Interpreting eigenspectra
-고유스펙트럼 해석하기

	PCA에서 하는 일은 고유값의 스펙트럼을 살펴보는 것.
	고유값을 정렬한 다음, 그것들을 성분번호에 따른 함수로 플롯

![199.Pasted image 20241014082756](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014082756.png)

	스펙트럼에서 크게 튀어나온 고유값들을 찾는것이 중요
	
	초반 2~3개가 스펙트럼에서 확연히 두드러짐
	-> 이러한 값들이  데이터를 나타내는 중요한 성분으로 간주됨

---
# <span style="color:rgb(255, 192, 0)">Two limitations of PCA</span><span style="color:rgb(205, 205, 81)">(these are not necessarily disadvantages)</span> 

	실제로는 데이터를 압축한 후 결과가 더 혼란스럽고 원래의 고차원 공간의 데이터보다 덜 합리적으로 보일 수 있음

## Limitation #1: PCs are orthogonal
-주성분이 직교한다.

 ![199.Pasted image 20241014083224](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014083224.png)

	직교 
		기하학적 해석 :두 벡터가 90도 각도로 만날 때를 의미
		 대수적 해석 : 두 주성분 사이의 상관관계는 반드시 0이어야 한다.

	상관관계가 작다는 것이 아닌, 주성분들이 정확히 상관관계가 0이 되도록 강제하는 수학적 제약이 있음

![199.Pasted image 20241014083805](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014083805.png)

	주성분분석(PCA)와 독립성분분석(ICA)는 서로 다른 방향을 보임
	둘 중 어느것이 틀리거나 맞는것이 아닌, 서로 다른 특성을 보여주는것

## Limitation #2: PCA: variance = relevance
-분산 = 관련성

	데이터에 어떤 특징들이 있던간에, 분산이 가장 크고 변수들 간 공분산이 가장 큰 특징만을 PCA가 잡아냄
	-> 데이터에 정말의미 있고중요한 특징이 있더라도 그 패턴이 데이터 특성이나 소수의 열에 한정된다면, PCA는 무시하게 될 수 있음.

	PCA는 일반적이지 않은 값들을 억제함. 그런데 이 일반적이지 않은 것들이 가장 중요한 요소일 수 있음.

![199.Pasted image 20241014084704](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014084704.png)

<span style="color:rgb(205, 205, 81)">가장 중요한 패턴은 큰 분산이 아닐 수 있음</span> 

	위 그래프에서, 데이터들의 색상 차원일 때, PCA는 실제로 색상 차원을 분리해낼 수 없음. PCA는 큰 구조에만 집중함.

	데이터에따라서 가장 큰 분산의 원인이 실제로는 노이즈나 잡음일 수 있음.

PCA는 데이터가 PCA의 기본 가정을 충족할 떄 매우 강력한 방법이 될 수 있음.

---
## Conclusion of PCA

<span style="color:rgb(116, 195, 194)">PCA는 차원 압축, 차원 축소, 고차원 데이터의 시각적 탐구에 매우 적합하다.</span> 

<span style="color:rgb(230, 122, 122)">PCA는 주성분을 데이터 내 요인 또는 고유한 분산의 출처로 해석할 때, 최적화되지 않거나 오해를 불러일으킬 수 있다.</span> 
	PCA가 단순히 데이터를 요인이나 고유한 분산으로 나누는 것이 항상 정확한 해석을 제공하지 않을 수 있음


---
## Code

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
```

```python
## create the data

N = 1000

# data
x = np.array([np.random.randn(N),.4*np.random.randn(N)]).T

# rotation matrix
th = np.pi/4
R1 = [[np.cos(th),-np.sin(th)],[np.sin(th),np.cos(th)]]

# rotate data
y = x@np.array(R1)

axlim = [-1.1*max(abs(y.flatten())),1.1*max(abs(y.flatten()))] # axis limits

# and plot
plt.plot(y[:,0],y[:,1],'k.')
plt.xticks([])
plt.yticks([])
plt.xlabel('$X_1$')
plt.ylabel('$X_2$')
plt.axis('square')
plt.title('Data space')
plt.show()
```
![199.Pasted image 20241014183636](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014183636.png)


```python
## now for PCA

# PCA using scikitlearn's function
pca = PCA().fit(y)

# get the PC scores
pcscores  = pca.transform(y)

# and plot
fig,ax = plt.subplots(1,2,figsize=(8,4))
ax[0].plot(y[:,0],y[:,1],'k.')
ax[0].axis('square')
ax[0].set_xticks([])
ax[0].set_yticks([])
ax[0].set_xlim(axlim)
ax[0].set_ylim(axlim)
ax[0].set_xlabel('$X_1$')
ax[0].set_ylabel('$X_2$')
ax[0].set_title('Data space')

ax[1].plot(pcscores[:,0],pcscores[:,1],'k.')
ax[1].axis('square')
ax[1].set_xticks([])
ax[1].set_yticks([])
ax[1].set_xlim(axlim)
ax[1].set_ylim(axlim)
ax[1].set_xlabel('$X_1$')
ax[1].set_ylabel('$X_2$')
ax[1].set_title('PC space')
```
![199.Pasted image 20241014183650](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014183650.png)

```python
## for dimension reduction 

spikes = np.loadtxt('spikes.csv',delimiter=',')

# let's see it!
plt.plot(np.mean(spikes,axis=0))
#plt.plot(spikes)
plt.title('Average of all spikes')
plt.show()

plt.imshow(spikes,aspect='auto')
plt.xlabel('Time points')
plt.ylabel('Individual spikes')
plt.show()
```
![199.Pasted image 20241014183705](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014183705.png)
![199.Pasted image 20241014183715](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014183715.png)


```python
## pca

# PCA using scikitlearn's function
pca = PCA().fit(spikes)

# get the PC scores and the eigenspectrum
pcscores = pca.transform(spikes)
explVar = pca.explained_variance_ # 고유값 / PCA를 사용할 때 몇개의 주성을분을 선택할지에 대한 기준/ 각 주성분의 분산
explVar = 100*explVar/sum(explVar) # convert to %total
coeffs = pca.components_ # 고유벡터 / 각 특성이 해당 주성분에 영향을 미치는지 정도를 의미


# show the scree plot (a.k.a. eigenspectrum)
fig,ax = plt.subplots(1,2,figsize=(10,4))

ax[0].plot(explVar,'kp-',markerfacecolor='k',markersize=10)
ax[0].set_xlabel('Component number')
ax[0].set_ylabel('Percent variance explained')

ax[1].plot(np.cumsum(explVar),'kp-',markerfacecolor='k',markersize=10)
ax[1].set_xlabel('Cimponent number')
ax[1].set_ylabel('Cumulative percent varicance explained')
plt.show()

# now show the PC weights for the top two components
plt.plot(coeffs[0,:])
plt.plot(coeffs[1,:])
plt.xlabel('Time')
plt.legend(('Comp 1','Comp 2'))
plt.title('PC weights (coefficients)')
plt.show()
```
![199.Pasted image 20241014183735](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014183735.png)
![199.Pasted image 20241014183747](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014183747.png)


```python
## finally, show the PC scores
plt.plot(pcscores[:,0],pcscores[:,1],'k.',markersize=.1)
plt.xlabel('PC_1')
plt.ylabel('PC_2')
plt.title('PC space')
plt.show()
```
![199.Pasted image 20241014183758](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241014183758.png)

---
## Unsupervised

```python
from sklearn.cluster import KMeans

## k-means clustering
k = 2
data = np.vstack((pcscores[:,0],pcscores[:,1])).T #하나의 데이터로 합친 후 분리해야하므로 전치해야함
kmeans = KMeans(n_clusters=k)
kmeans = kmeans.fit(data)
# group labels
cents = kmeans.cluster_centers_
y_pred = kmeans.fit_predict(data)

plt.scatter(data[:, 0], data[:, 1], c=y_pred, s=.1)
plt.plot(cents[:,0],cents[:,1],'ko')
plt.title("K-means on PC data")
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.show()
```
![199.Pasted image 20241015083456](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241015083456.png)

```python
plt.plot(spikes[y_pred==0].mean(axis=0))
plt.plot(spikes[y_pred==1].mean(axis=0))
plt.xlabel('Time')
plt.legend(('Neuron 1','Neuron 2')) 
plt.title('PC weights (coefficients)')
plt.show()
```
![199.Pasted image 20241015083518](../pic/16.%20Clustrering%20and%20dimension-reduction/199.Pasted%20image%2020241015083518.png)
