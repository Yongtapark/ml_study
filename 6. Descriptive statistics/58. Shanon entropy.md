- 어떻게 엔트로피를 연산하는지
- 어떻게 엔트로피를 해석하는지
----
#### What's in a name?
사건의 불확실성을 수량화 하는 방법
불확실성이 클수록 더 많은 정보가 존재하게 된다.
불확실성이 증가할수록 엔트로피도 증가한다.
얼마나 불확실한가를 수치로 나타낸것
![[58.entropy.png]]
- 경기를 한다고 할때, 이 경기가 이길지, 질지를 알게된다면, 그것은 즉, 승부가 어떻게 날지 불확실한게 아니라, 확실해져가고 있는것이다. 즉, 누가 이길지 알수 없을때가 엔트로피가 가장 높고, 어느 한쪽이 지든 이기든 확실해져 간다면, 엔트로피는 줄어든다.
---
#### Formula for entropy
## $$H = -\sum^n_{i=1}p(x_i)log_2(p(x_i))$$
x = data values
p = probability
** 1보다 작은 수의 로그는 음수를 반환하기 때문에, 모든것이 음수가 된다. 그 상태에서 그래프를그리면 U자 형태가 된다.
** 음수로 계산하는것보다는 양수로 계산하는게 더 좋기때문에 -를 붙여 수를 양수로 변환한다.
** 이전의 엔트리 그래프가 ∩ 모양인 이유가 -를 곱해서 U가 뒤집어졌기 때문이다.

<span style="color:rgb(255, 192, 0)">어떤 데이터에 이 공식을 적용할 수 있을까?</span>
<span style="color:rgb(205, 205, 81)">명목,순서,이산 데이터</span>

<span style="color:rgb(255, 192, 0)">구간,비율 데이터는 어떨까?</span> 
<span style="color:rgb(205, 205, 81)">히스토그램 생성을 통해 이산 데이터로 변환한다.</span> 
<span style="color:rgb(41, 194, 191)">중요: 엔트로피는 빈의 크기와 빈의 수에 따라 달라진다.</span> 

---
#### Interpreting entropy

<span style="color:rgb(255, 192, 0)">높은 엔트로피는 데이터셋이 많은 변동성을 가진다는것을 의미한다. -> 데이터가 일정하지 않고 다양하다 -> 정보가 많다.</span>
<span style="color:rgb(205, 205, 81)">낮은 엔트로피는 데이터셋의 값이 대부분 비슷하다는것을 의미한다. -> 값이 반복되므로 정보가 적다 -> 그래서 엔트로피가 낮으면 정보가 적다.</span>

<span style="color:rgb(236, 158, 111)">엔트로피와 분산의 차이는 뭘까?</span> 
- 분산과 엔트로피는 데이터의 유사한 특성을 측정한다는 공통점이 있다.
<span style="color:rgb(205, 205, 81)">엔트로피는 비선형이며, 분포에 대한 어떠한 가정도 하지 않는다.</span> 
- 분포가 어떻게 생겼는지 고민할 필요 없음
<span style="color:rgb(41, 194, 191)">분산은 평균의 유효성에 의존 -> 대략적인 정규 분포 데이터에만 적용이 가능함.</span> 

엔트로피에는 두가지 단위가 존재
<center><span style="color:rgb(255, 192, 0)">Units:"bits"</span></center>
## $$H = -\sum^n_{i=1}p(x_i)log_2(p(x_i))$$
- 최대 엔트로피 : 1

<center><span style="color:rgb(255, 192, 0)">Units:"nats"</span></center>
## $$H = -\sum^n_{i=1}p(x_i)ln(p(x_i))$$
- 자연로그 기반

이 두 공식은 단위만 일관적으로 사용하면 어떤 공식을 사용해도 상관없음

---
#### Code

1~8사이의 값들을  생성
```python
## discrete entropy

# generate data
N = 1000
numbers = np.ceil(8*np.random.rand(N)**2)
#numbers[numbers==7]=4
plt.plot(numbers,'o')
```
![[58.data_gen.png]]

고유한 값들을 찾고, 각 값의 빈도를 계산한 후, 전체 데이터 크기로 나누어 확률을 구한다.
이후 bit unit 공식을 적용하여 엔트로피를 구한다 이 때, 
np.finfo(float).eps(엡실론)를 추가적으로 더해 에러를 방지해야한다.
- 만약, 1~8중 한 수의 값이 존재할 확률이 0이라면, 해당공식을 적용할 때, log(0)이 된다. 이러면 $-\infty$
가 되어버리므로 엔트로피 값을 계산할 수 없다. 때문에 0에 가까운 작은 수인 엡실론을 더해 엔트로피 값을 계산할수 있게 한다.
**Cumulative : 누적
```python
# get counts and probabilities
#u = [1,2,3,4,5,6,7,8]#np.unique(numbers)
u = np.unique(numbers)

probs = np.zeros(len(u))

for ui in range(len(u)):
    probs[ui] = sum(numbers==u[ui]) / N

# compute entropy
entropee = -sum(probs*np.log2(probs+np.finfo(float).eps))
print(np.finfo(float).eps)
# plot
plt.bar(u,probs)
plt.title('Entropy = %g'%entropee)
plt.xlabel('Data value')
plt.ylabel('Probability')
plt.show()
```
![[58.bar.png]]

엔트로피 계산
nbins 값을 조정하면 엔트로피가 변한다. -> 유의

```python
### now compute entropy
# number of bins
nbins = 50

# bin the data and convert to probability
nPerBin,bins = np.histogram(brownnoise,nbins) #nPerBin = 각 bin에 속하는 데이터 포인트의 개수 / bins = 히스토그램의 경계값
probs = nPerBin / sum(nPerBin)

# compute entropy
entro = -sum(probs*np.log2(probs+np.finfo(float).eps))

print('Entropy = %g'%entro)
```
```
Entropy = 5.38198
```