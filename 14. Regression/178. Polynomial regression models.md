- 다항 회귀의 형태
- 다항 회귀의 계수를 피팅하는 방법
- 베이즈 정보 기준을 사용하여 적절한 모델 차수를 설정 하는 방법
---
## Reminder of polynomials

![178.Pasted image 20241003080910](../pic/14.%20Regression/178.Pasted%20image%2020241003080910.png)
	x는 점점 높은 거듭제곱으로 표현됨
	n차 다항식

![178.Pasted image 20241003083503](../pic/14.%20Regression/178.Pasted%20image%2020241003083503.png)
	예시
	3차 다항식

![178.Pasted image 20241003083527](../pic/14.%20Regression/178.Pasted%20image%2020241003083527.png)
	다항식에는 모든 X항이 꼭 들어갈 필요는 없음
	다항식의 차수는 항의 총 개수를 말하는것이 아닌 각 항 중에서 가장 큰 지수에 해당함
	3차 다항식

---
## Polynomial regression

![178.Pasted image 20241003084014](../pic/14.%20Regression/178.Pasted%20image%2020241003084014.png)
	1차 다항 회귀

![178.Pasted image 20241003084112](../pic/14.%20Regression/178.Pasted%20image%2020241003084112.png)
	k차 다항 회귀

#### <span style="color:rgb(236, 158, 111)">어떻게 이런 종류의 비선형성 모델을 피팅할 수 있는가?</span> 

![178.Pasted image 20241003084318](../pic/14.%20Regression/178.Pasted%20image%2020241003084318.png)
	X 자채는 선형이지만, 높은 차수로 올리고있음.
	하지만 회귀는 일반적인 선형 모델에 관한 것이며, 따라서 선형이어야 하지 않은가?

#### <span style="color:rgb(118, 147, 234)">상관계수는 모두 선형이다! 이것은 일반 선형 모델이다!</span> 

	계수들은 모두 선형이다. 베타 계수들은 모두선형적이며 단순히 곱해지고 있을 뿐
	계수에 대해 비선형적인 처리를 하고 있지 않기 때문
		단지 데이터 자체를 더 높은 차수로 올리고 있을 뿐

---
## When would you use a polynomial regression?

![178.Pasted image 20241003085914](../pic/14.%20Regression/178.Pasted%20image%2020241003085914.png)
	이러한 유형의 데이터가 다항 회귀를 사용하는 경우

##### 그렇다면 적절한 차수는 무엇일까?
-다항식의 차수를 어떻게 정할 수 있을까?

---
## Which order should you use?

![178.Pasted image 20241003091005](../pic/14.%20Regression/178.Pasted%20image%2020241003091005.png)
	다항 회귀에서 적절한 차수를 찾는 것이 중요
	차수가 너무 작으면 데이터에 적합하지 않고, 너무 크면 과적합이 발생함
	따라서 데이터를 잘 표현하는 최적의 차수를 찾아야 함
		적절한 차수를 차즌 방법은 베이즈 정보 기준(Bayes Information Criteria/BIC)이라는것을 통해 얻음 

---
## Model order selection, thanks to Bayes

![178.Pasted image 20241003091649](../pic/14.%20Regression/178.Pasted%20image%2020241003091649.png)

$n\ln(SS_\epsilon)$ : 잔차제곱합의 자연로그에 n을 곱함
n : 데이터 포인트의 개수
$\ln$ : 자연로그 

$k\ln(n)$ -> 여기서는 왜 자연로그에 k를 곱합
k : 파라미터 개수

	이제 우리가 할 일은 다양한 차수으ㅔ 모델을 모두 실행하여 적합시키는것.
	그리고 이 모델들의 BIC를 플로팅

![178.Pasted image 20241003093248](../pic/14.%20Regression/178.Pasted%20image%2020241003093248.png)
	여기서 우리가 찾아야 하는것은 BIC값이 최소로 나오는 곳
	최소값이 최적의 모델 차수임

---
## Code 

```python
## generate the data

n = 30
x = np.linspace(-2,4,n)
y1 = x**2 + np.random.randn(n)
y2 = x**3 + np.random.randn(n)

# plot the data
plt.plot(x,y1,'ko',markerfacecolor='r')
plt.plot(x,y2,'ko',markerfacecolor='g')
plt.legend(('Quadratic','Cubic'))
plt.show()
```
![178.Pasted image 20241003101030](../pic/14.%20Regression/178.Pasted%20image%2020241003101030.png)

```python
## now for a polynomial fit

# for y1
# pterms : 다항식 항. 2차다항식의 경우, x^2, x^1, x^0 값 순으로 출력. 즉 절편과 2개의 항을 얻음
# -> 다항식 회귀 계수를 얻음
pterms = np.polyfit(x,y1,2)
yHat1 = np.polyval(pterms,x)

# % for y2
pterms = np.polyfit(x,y2,3)
yHat2 = np.polyval(pterms,x)

# and all the plotting

plt.plot(x,y1,'ko',markerfacecolor='r',label='y1')
plt.plot(x,y2,'ko',markerfacecolor='g',label='y2')

plt.plot(x,yHat1,'r', label='y1 fit')
plt.plot(x,yHat2,'g', label='y2 fit')
plt.legend()
plt.show()
```
![178.Pasted image 20241003101059](../pic/14.%20Regression/178.Pasted%20image%2020241003101059.png)

```python
# compute R2

# compute R2 for several polynomial orders
orders = np.arange(1,6)

# output matrices
r2 = np.zeros((2,len(orders)))
sse = np.zeros((2,len(orders)))

# the loop!
for oi in range(len(orders)):

    # fit the model with oi terms
    pterms = np.polyfit(x,y1,orders[oi])
    yHat = np.polyval(pterms,x)

    # compute R2
    ss_eta = sum((y1-yHat)**2) # numerator
    ss_tot = sum((y1-np.mean(y1))**2) # denominator
    r2[0,oi] = 1 - ss_eta/ss_tot # R^2
    sse[0,oi] = ss_eta # store just the SSe for mddel comparison later


    # repeat for y2
    pterms = np.polyfit(x,y2,orders[oi])
    yHat = np.polyval(pterms,x)
    ss_eta = sum((y2-yHat)**2) # numerator
    ss_tot = sum((y2-np.mean(y1))**2) # denominator
    r2[1,oi] = 1 - ss_eta/ss_tot # R^2
    sse[1,oi] = ss_eta # store just the SSe for mddel comparison later


```

```python
fig, ax = plt.subplots(2,1,figsize=(6,4))

# plot the R2 results
ax[0].plot(orders,r2[0,:],'s-',markerfacecolor='w')
ax[0].plot(orders,r2[1,:],'s-',markerfacecolor='w')
ax[0].set_ylabel('Model $R^2$')
ax[0].set_xticks([])
ax[0].legend(('y1','y2'))

# compute the Bayes Information Criteraion
bic = n*np.log(sse) + orders*np.log(n)
ax[1].plot(orders,bic[0,:],'s-',markerfacecolor='w')
ax[1].plot(orders,bic[1,:],'s-',markerfacecolor='w')
ax[1].set_xlabel('Polynomial nodel order')
ax[1].set_xticks(range(1,5))
ax[1].set_ylabel('Model BIC')


#optional zoom
#ax[1].set_ylim([90,120])

plt.show()

# r2그래프를 보면 차수가 높아질수록 r2 값이 점점 더 높아진다는 것.
# 즉, 차수 파라미터를 늘리면 모델에 더 많은 파라미터를 넣게 되고, 모델이 데이터에 더욱 적합하게 됨. -> 과적합의 위험
# r2 만으로는 바른 파라미터의 수나 최적의 파라미터 수를 결정 할 수 없음

# BIC의 경우, 파라미터의 수가 너무 적을 때, BIC 값이 매우 크게 나타나는 경향이 있음
# 그리고 최적 파라미터 수를 넘어서면 다시 값이 커지지만, 그 증가 속도가 매우 
# 때문에 올바른 파라미터 수를 파악하려면 축소해서 자세히 볼 필요가 있음

```
![178.Pasted image 20241003101119](../pic/14.%20Regression/178.Pasted%20image%2020241003101119.png)

---
## Code2 : polyfit, polyval 없이 다항회귀 구하기

polyfit, polyval 없이 다항회귀 구현
최소자승법(OLS)를 구현하여 베타값을 얻고, 이를 이용하여 예측값을 얻는다.

1. 2차, 3차항으로 만들어진 데이터를 전부 합친 X를 생성
2. 이후 베타계수를 얻기 위해 $\beta = (X^TX)^{-1}X^Ty$ 구현
3. 이차항의 경우 $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2$ 이므로, 행렬곱으로 구현

데이터 전체를 다루는 수식에서는 행렬곱

개별 데이터를 다루는 수식에서는 일반 곱셈

```python
import statsmodels.api as sm

# 설계 행렬 만들기 (2차 다항식의 경우)
X = np.vstack((np.ones(len(x)), x, x**2)).T
# 이 반환값은 기존의 Polyval과는 순서가 반대로임 
quardratic_term_beta = np.linalg.inv(X.T@X)@(X.T@y1)
#$y_hat = 계수*데이터^차수의 합
y1hat =X@quardratic_term_beta

#반복
X = np.vstack((np.ones(len(x)), x, x**2,x**3)).T
cube_term_beta = np.linalg.inv(X.T@X)@(X.T@y2)
y2hat = X@cube_term_beta


# and all the plotting

plt.plot(x,y1,'ko',markerfacecolor='r',label='y1')
plt.plot(x,y2,'ko',markerfacecolor='g',label='y2')

plt.plot(x,y1hat,'r', label='y1 fit')
plt.plot(x,y2hat,'g', label='y2 fit')
plt.legend()
plt.show()
```
![178.Pasted image 20241003101059](../pic/14.%20Regression/178.Pasted%20image%2020241003101059.png)

```python
# compute R2

# compute R2 for several polynomial orders
orders = np.arange(1,6)

# output matrices
r2 = np.zeros((2,len(orders)))
sse = np.zeros((2,len(orders)))

# the loop!
for oi in range(len(orders)):
    
    # fit the model with oi terms for y1
    X1 = np.vstack([x**i for i in range(orders[oi] + 1)]).T
    pterms = np.linalg.inv(X1.T @ X1) @ (X1.T @ y1)
    yHat = X1 @ pterms

    # compute R2 for y1
    ss_eta = sum((y1 - yHat) ** 2)  # numerator
    ss_tot = sum((y1 - np.mean(y1)) ** 2)  # denominator
    r2[0, oi] = 1 - ss_eta / ss_tot  # R^2
    sse[0, oi] = ss_eta  # store just the SSe for model comparison later

    # fit the model with oi terms for y2
    X2 = np.vstack([x**i for i in range(orders[oi] + 1)]).T
    pterms = np.linalg.inv(X2.T @ X2) @ (X2.T @ y2)
    yHat = X2 @ pterms

    # compute R2 for y2
    ss_eta = sum((y2 - yHat) ** 2)  # numerator
    ss_tot = sum((y2 - np.mean(y2)) ** 2)  # denominator
    r2[1, oi] = 1 - ss_eta / ss_tot  # R^2
    sse[1, oi] = ss_eta  #


```

```python
fig, ax = plt.subplots(2,1,figsize=(6,4))

# plot the R2 results
ax[0].plot(orders,r2[0,:],'s-',markerfacecolor='w')
ax[0].plot(orders,r2[1,:],'s-',markerfacecolor='w')
ax[0].set_ylabel('Model $R^2$')
ax[0].set_xticks([])
ax[0].legend(('y1','y2'))

# compute the Bayes Information Criteraion
bic = n*np.log(sse) + orders*np.log(n)
ax[1].plot(orders,bic[0,:],'s-',markerfacecolor='w')
ax[1].plot(orders,bic[1,:],'s-',markerfacecolor='w')
ax[1].set_xlabel('Polynomial nodel order')
ax[1].set_xticks(range(1,5))
ax[1].set_ylabel('Model BIC')


#optional zoom
#ax[1].set_ylim([90,120])

plt.show()
```
![178.Pasted image 20241003101119](../pic/14.%20Regression/178.Pasted%20image%2020241003101119.png)
